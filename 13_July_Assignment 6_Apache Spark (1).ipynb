{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 1. Working with RDDs:\n",
    "   a) Write a Python program to create an RDD from a local data source.\n",
    "   b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
    "   c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate.\n",
    "\n",
    "\n",
    "Solution:-\n",
    "    \n",
    "a) Creating RDDs from Local Collections:\n",
    "One of the simplest ways to create an RDD is from a local collection in memory. Spark provides the parallelize method, which takes a collection and distributes it across the cluster. Let's consider an example where we have a list of numbers and want to create an RDD from it:\n",
    "\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "// Create a SparkContext\n",
    "val conf = new SparkConf().setAppName(\"RDD Creation Example\").setMaster(\"local\")\n",
    "val sc = new SparkContext(conf)\n",
    "\n",
    "// Create an RDD from a local collection\n",
    "val data = Array(1, 2, 3, 4, 5)\n",
    "val rdd = sc.parallelize(data)\n",
    "\n",
    "// Perform operations on the RDD\n",
    "// For example, let's calculate the sum of the numbers\n",
    "val sumOfNumbers = rdd.sum()\n",
    "println(\"Sum of numbers: \" + sumOfNumbers)\n",
    "\n",
    "// Stop the SparkContext\n",
    "sc.stop()\n",
    "\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"RDD Creation Example\")\n",
    "\n",
    "# Create an RDD from a local collection\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Perform operations on the RDD\n",
    "# For example, let's calculate the sum of the numbers\n",
    "sum_of_numbers = rdd.sum()\n",
    "print(\"Sum of numbers:\", sum_of_numbers)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n",
    "Creating RDDs from Files:\n",
    "Spark supports reading data from various file formats such as text files, CSV files, JSON files, and more. Let’s consider an example where we have a text file containing customer information, and we want to create an RDD from it:\n",
    "\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "// Create a SparkContext\n",
    "val conf = new SparkConf().setAppName(\"RDD Creation Example\").setMaster(\"local\")\n",
    "val sc = new SparkContext(conf)\n",
    "\n",
    "// Create an RDD from a text file\n",
    "val rdd = sc.textFile(\"customer_data.txt\")\n",
    "\n",
    "// Perform operations on the RDD\n",
    "// For example, let's count the number of lines\n",
    "val lineCount = rdd.count()\n",
    "println(\"Number of lines: \" + lineCount)\n",
    "\n",
    "// Stop the SparkContext\n",
    "sc.stop()\n",
    "\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"RDD Creation Example\")\n",
    "\n",
    "# Create an RDD from a text file\n",
    "rdd = sc.textFile(\"customer_data.txt\")\n",
    "\n",
    "# Perform operations on the RDD\n",
    "# For example, let's count the number of lines\n",
    "line_count = rdd.count()\n",
    "print(\"Number of lines:\", line_count)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120fe8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    " b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
    "    \n",
    "Solution:- Spark RDD Operations-Transformation & Action with Example\n",
    "\n",
    "1. Spark RDD Operations\n",
    "Two types of Apache Spark RDD operations are- Transformations and Actions. \n",
    "A Transformation is a function that produces new RDD from the existing RDDs but when we want to work with the actual dataset, at that point Action is performed. When the action is triggered after the result,\n",
    "new RDD is not formed like transformation. \n",
    "In this Apache Spark RDD operations tutorial we will get the detailed view of what is Spark RDD,\n",
    "what is the transformation in Spark RDD, various RDD transformation operations in Spark with examples, \n",
    "what is action in Spark RDD and various RDD action operations in Spark with examples.\n",
    "\n",
    "2. Apache Spark RDD Operations\n",
    "Before we start with Spark RDD Operations, let us deep dive into RDD in Spark.\n",
    "Apache Spark RDD supports two types of Operations-\n",
    "\n",
    "Transformations\n",
    "Actions\n",
    "Now let us understand first what is Spark RDD Transformation and Action-\n",
    "\n",
    "3. RDD Transformation\n",
    "Spark Transformation is a function that produces new RDD from the existing RDDs. It takes RDD as input and produces one or more RDD as output. Each time it creates new RDD when we apply any transformation. Thus, the so input RDDs, cannot be changed since RDD are immutable in nature.\n",
    "\n",
    "Applying transformation built an RDD lineage, with the entire parent RDDs of the final RDD(s). RDD lineage, also known as RDD operator graph or RDD dependency graph. It is a logical execution plan i.e., it is Directed Acyclic Graph (DAG) of the entire parent RDDs of RDD.\n",
    "\n",
    "Transformations are lazy in nature i.e., they get execute when we call an action. They are not executed immediately. Two most basic type of transformations is a map(), filter().\n",
    "After the transformation, the resultant RDD is always different from its parent RDD. It can be smaller (e.g. filter, count, distinct, sample), bigger (e.g. flatMap(), union(), Cartesian()) or the same size (e.g. map).\n",
    "\n",
    "There are two types of transformations:\n",
    "\n",
    "Narrow transformation – In Narrow transformation, all the elements that are required to compute the records in single partition live in the single partition of parent RDD. A limited subset of partition is used to calculate the result. Narrow transformations are the result of map(), filter().\n",
    "\n",
    "Wide transformation – In wide transformation, all the elements that are required to compute the records in the single partition may live in many partitions of parent RDD. The partition may live in many partitions of parent RDD. Wide transformations are the result of groupbyKey() and reducebyKey().\n",
    "\n",
    "There are various functions in RDD transformation. Let us see RDD transformation with examples.\n",
    "\n",
    "3.1. map(func)\n",
    "The map function iterates over every line in RDD and split into new RDD. Using map() transformation we take in any function, and that function is applied to every element of RDD.\n",
    "\n",
    "In the map, we have the flexibility that the input and the return type of RDD may differ from each other. For example, we can have input RDD type as String, after applying the\n",
    "\n",
    "map() function the return RDD can be Boolean.\n",
    "\n",
    "For example, in RDD {1, 2, 3, 4, 5} if we apply “rdd.map(x=>x+2)” we will get the result as (3, 4, 5, 6, 7).\n",
    "\n",
    "Also Read: How to create RDD\n",
    "\n",
    "Map() example:\n",
    "\n",
    "[php]import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "object  mapTest{\n",
    "def main(args: Array[String]) = {\n",
    "val spark = SparkSession.builder.appName(“mapExample”).master(“local”).getOrCreate()\n",
    "val data = spark.read.textFile(“spark_test.txt”).rdd\n",
    "val mapFile = data.map(line => (line,line.length))\n",
    "mapFile.foreach(println)\n",
    "}\n",
    "}[/php]\n",
    "\n",
    "spark_test.txt”\n",
    "\n",
    "hello...user! this file is created to check the operations of spark.\n",
    "?, and how can we apply functions on that RDD partitions?. All this will be done through spark programming which is done with the help of scala language support…\n",
    "Note – In above code, map() function map each line of the file with its length.\n",
    "3.2. flatMap()\n",
    "With the help of flatMap() function, to each input element, we have many elements in an output RDD. The most simple use of flatMap() is to split each input string into words.\n",
    "Map and flatMap are similar in the way that they take a line from input RDD and apply a function on that line. The key difference between map() and flatMap() is map() returns only one element, while flatMap() can return a list of elements.\n",
    "\n",
    "flatMap() example:\n",
    "\n",
    "[php]val data = spark.read.textFile(“spark_test.txt”).rdd\n",
    "val flatmapFile = data.flatMap(lines => lines.split(” “))\n",
    "flatmapFile.foreach(println)[/php]\n",
    "\n",
    "Note – In above code, flatMap() function splits each line when space occurs.\n",
    "3.3. filter(func)\n",
    "Spark RDD filter() function returns a new RDD, containing only the elements that meet a predicate. It is a narrow operation because it does not shuffle data from one partition to many partitions.\n",
    "\n",
    "For example, Suppose RDD contains first five natural numbers (1, 2, 3, 4, and 5) and the predicate is check for an even number. The resulting RDD after the filter will contain only the even numbers i.e., 2 and 4.\n",
    "\n",
    "Filter() example:\n",
    "\n",
    "[php]val data = spark.read.textFile(“spark_test.txt”).rdd\n",
    "val mapFile = data.flatMap(lines => lines.split(” “)).filter(value => value==”spark”)\n",
    "println(mapFile.count())[/php]\n",
    "\n",
    "Note – In above code, flatMap function map line into words and then count the word “Spark” using count() Action after filtering lines containing “Spark” from mapFile.\n",
    "Read: Apache Spark RDD vs DataFrame vs DataSet\n",
    "\n",
    "3.4. mapPartitions(func)\n",
    "The MapPartition converts each partition of the source RDD into many elements of the result (possibly none). In mapPartition(), the map() function is applied on each partitions simultaneously. MapPartition is like a map, but the difference is it runs separately on each partition(block) of the RDD.\n",
    "\n",
    "3.5. mapPartitionWithIndex()\n",
    "It is like mapPartition; Besides mapPartition it provides func with an integer value representing the index of the partition, and the map() is applied on partition index wise one after the other.\n",
    "\n",
    "Learn: Spark Shell Commands to Interact with Spark-Scala\n",
    "\n",
    "3.6. union(dataset)\n",
    "With the union() function, we get the elements of both the RDD in new RDD. The key rule of this function is that the two RDDs should be of the same type.\n",
    "For example, the elements of RDD1 are (Spark, Spark, Hadoop, Flink) and that of RDD2 are (Big data, Spark, Flink) so the resultant rdd1.union(rdd2) will have elements (Spark, Spark, Spark, Hadoop, Flink, Flink, Big data).\n",
    "\n",
    "Union() example:\n",
    "\n",
    "[php]val rdd1 = spark.sparkContext.parallelize(Seq((1,”jan”,2016),(3,”nov”,2014),(16,”feb”,2014)))\n",
    "val rdd2 = spark.sparkContext.parallelize(Seq((5,”dec”,2014),(17,”sep”,2015)))\n",
    "val rdd3 = spark.sparkContext.parallelize(Seq((6,”dec”,2011),(16,”may”,2015)))\n",
    "val rddUnion = rdd1.union(rdd2).union(rdd3)\n",
    "rddUnion.foreach(Println)[/php]\n",
    "\n",
    "Note – In above code union() operation will return a new dataset that contains the union of the elements in the source dataset (rdd1) and the argument (rdd2 & rdd3).\n",
    "3.7. intersection(other-dataset)\n",
    "With the intersection() function, we get only the common element of both the RDD in new RDD. The key rule of this function is that the two RDDs should be of the same type.\n",
    "Consider an example, the elements of RDD1 are (Spark, Spark, Hadoop, Flink) and that of RDD2 are (Big data, Spark, Flink) so the resultant rdd1.intersection(rdd2) will have elements (spark).\n",
    "\n",
    "Intersection() example:\n",
    "\n",
    "[php]val rdd1 = spark.sparkContext.parallelize(Seq((1,”jan”,2016),(3,”nov”,2014, (16,”feb”,2014)))\n",
    "val rdd2 = spark.sparkContext.parallelize(Seq((5,”dec”,2014),(1,”jan”,2016)))\n",
    "val comman = rdd1.intersection(rdd2)\n",
    "comman.foreach(Println)[/php]\n",
    "\n",
    "Note – The intersection() operation return a new RDD. It contains the intersection of elements in the rdd1 & rdd2.\n",
    "Learn to Install Spark on Ubuntu\n",
    "\n",
    "3.8. distinct()\n",
    "It returns a new dataset that contains the distinct elements of the source dataset. It is helpful to remove duplicate data.\n",
    "For example, if RDD has elements (Spark, Spark, Hadoop, Flink), then rdd.distinct() will give elements (Spark, Hadoop, Flink).\n",
    "\n",
    "Distinct() example:\n",
    "\n",
    "[php]val rdd1 = park.sparkContext.parallelize(Seq((1,”jan”,2016),(3,”nov”,2014),(16,”feb”,2014),(3,”nov”,2014)))\n",
    "val result = rdd1.distinct()\n",
    "println(result.collect().mkString(“, “))[/php]\n",
    "\n",
    "Note – In the above example, the distinct function will remove the duplicate record i.e. (3,'”nov”,2014).\n",
    "3.9. groupByKey()\n",
    "When we use groupByKey() on a dataset of (K, V) pairs, the data is shuffled according to the key value K in another RDD. In this transformation, lots of unnecessary data get to transfer over the network.\n",
    "\n",
    "Spark provides the provision to save data to disk when there is more data shuffled onto a single executor machine than can fit in memory. Follow this link to learn about RDD Caching and Persistence mechanism in detail.\n",
    "\n",
    "groupByKey() example:\n",
    "\n",
    "[php]val data = spark.sparkContext.parallelize(Array((‘k’,5),(‘s’,3),(‘s’,4),(‘p’,7),(‘p’,5),(‘t’,8),(‘k’,6)),3)\n",
    "val group = data.groupByKey().collect()\n",
    "group.foreach(println)[/php]\n",
    "\n",
    "Note – The groupByKey() will group the integers on the basis of same key(alphabet). After that collect() action will return all the elements of the dataset as an Array.\n",
    "3.10. reduceByKey(func, [numTasks])\n",
    "When we use reduceByKey on a dataset (K, V), the pairs on the same machine with the same key are combined, before the data is shuffled.\n",
    "\n",
    "reduceByKey() example:\n",
    "\n",
    "[php]val words = Array(“one”,”two”,”two”,”four”,”five”,”six”,”six”,”eight”,”nine”,”ten”)\n",
    "val data = spark.sparkContext.parallelize(words).map(w => (w,1)).reduceByKey(_+_)\n",
    "data.foreach(println)[/php]\n",
    "\n",
    "Note – The above code will parallelize the Array of String. It will then map each word with count 1, then reduceByKey will merge the count of values having the similar key.\n",
    "Read: Various Features of RDD\n",
    "\n",
    "3.11. sortByKey()\n",
    "When we apply the sortByKey() function on a dataset of (K, V) pairs, the data is sorted according to the key K in another RDD.\n",
    "\n",
    "sortByKey() example:\n",
    "\n",
    "[php] val data = spark.sparkContext.parallelize(Seq((“maths”,52), (“english”,75), (“science”,82), (“computer”,65), (“maths”,85)))\n",
    "val sorted = data.sortByKey()\n",
    "sorted.foreach(println)[/php]\n",
    "\n",
    "Note – In above code, sortByKey() transformation sort the data RDD into Ascending order of the Key(String).\n",
    "Read: Limitations of RDD\n",
    "\n",
    "3.12. join()\n",
    "The Join is database term. It combines the fields from two table using common values. join() operation in Spark is defined on pair-wise RDD. Pair-wise RDDs are RDD in which each element is in the form of tuples. Where the first element is key and the second element is the value.\n",
    "\n",
    "The boon of using keyed data is that we can combine the data together. The join() operation combines two data sets on the basis of the key.\n",
    "\n",
    "Join() example:\n",
    "\n",
    "[php]val data = spark.sparkContext.parallelize(Array((‘A’,1),(‘b’,2),(‘c’,3)))\n",
    "val data2 =spark.sparkContext.parallelize(Array((‘A’,4),(‘A’,6),(‘b’,7),(‘c’,3),(‘c’,8)))\n",
    "val result = data.join(data2)\n",
    "println(result.collect().mkString(“,”))[/php]\n",
    "\n",
    "Note –  The join() transformation will join two different RDDs on the basis of Key.\n",
    "Read: RDD lineage in Spark: ToDebugString Method\n",
    "\n",
    "3.13. coalesce()\n",
    "To avoid full shuffling of data we use coalesce() function. In coalesce() we use existing partition so that less data is shuffled. Using this we can cut the number of the partition. Suppose, we have four nodes and we want only two nodes. Then the data of extra nodes will be kept onto nodes which we kept.\n",
    "\n",
    "Coalesce() example:\n",
    "\n",
    "[php]val rdd1 = spark.sparkContext.parallelize(Array(“jan”,”feb”,”mar”,”april”,”may”,”jun”),3)\n",
    "val result = rdd1.coalesce(2)\n",
    "result.foreach(println)[/php]\n",
    "\n",
    "Note – The coalesce will decrease the number of partitions of the source RDD to numPartitions define in coalesce argument.\n",
    "4. RDD Action\n",
    "Transformations create RDDs from each other, but when we want to work with the actual dataset, at that point action is performed. When the action is triggered after the result, new RDD is not formed like transformation. Thus, Actions are Spark RDD operations that give non-RDD values. The values of action are stored to drivers or to the external storage system. It brings laziness of RDD into motion.\n",
    "\n",
    "An action is one of the ways of sending data from Executer to the driver. Executors are agents that are responsible for executing a task. While the driver is a JVM process that coordinates workers and execution of the task. Some of the actions of Spark are:\n",
    "\n",
    "4.1. count()\n",
    "Action count() returns the number of elements in RDD.\n",
    "\n",
    "For example, RDD has values {1, 2, 2, 3, 4, 5, 5, 6} in this RDD “rdd.count()” will give the result 8.\n",
    "\n",
    "Count() example:\n",
    "\n",
    "[php]val data = spark.read.textFile(“spark_test.txt”).rdd\n",
    "val mapFile = data.flatMap(lines => lines.split(” “)).filter(value => value==”spark”)\n",
    "println(mapFile.count())[/php]\n",
    "\n",
    "Note – In above code flatMap() function maps line into words and count the word “Spark” using count() Action after filtering lines containing “Spark” from mapFile.\n",
    "Learn: Spark Streaming\n",
    "\n",
    "4.2. collect()\n",
    "The action collect() is the common and simplest operation that returns our entire RDDs content to driver program. The application of collect() is unit testing where the entire RDD is expected to fit in memory. As a result, it makes easy to compare the result of RDD with the expected result.\n",
    "Action Collect() had a constraint that all the data should fit in the machine, and copies to the driver.\n",
    "\n",
    "Collect() example:\n",
    "\n",
    "[php]val data = spark.sparkContext.parallelize(Array((‘A’,1),(‘b’,2),(‘c’,3)))\n",
    "val data2 =spark.sparkContext.parallelize(Array((‘A’,4),(‘A’,6),(‘b’,7),(‘c’,3),(‘c’,8)))\n",
    "val result = data.join(data2)\n",
    "println(result.collect().mkString(“,”))[/php]\n",
    "\n",
    "Note – join() transformation in above code will join two RDDs on the basis of same key(alphabet). After that collect() action will return all the elements to the dataset as an Array.\n",
    "4.3. take(n)\n",
    "The action take(n) returns n number of elements from RDD. It tries to cut the number of partition it accesses, so it represents a biased collection. We cannot presume the order of the elements.\n",
    "\n",
    "For example, consider RDD {1, 2, 2, 3, 4, 5, 5, 6} in this RDD “take (4)” will give result { 2, 2, 3, 4}\n",
    "\n",
    "Take() example:\n",
    "\n",
    "[php]val data = spark.sparkContext.parallelize(Array((‘k’,5),(‘s’,3),(‘s’,4),(‘p’,7),(‘p’,5),(‘t’,8),(‘k’,6)),3)\n",
    "\n",
    "val group = data.groupByKey().collect()\n",
    "\n",
    "val twoRec = result.take(2)\n",
    "\n",
    "twoRec.foreach(println)[/php]\n",
    "\n",
    "Note – The take(2) Action will return an array with the first n elements of the data set defined in the taking argument.\n",
    "Learn: Apache Spark DStream (Discretized Streams)\n",
    "\n",
    "4.4. top()\n",
    "If ordering is present in our RDD, then we can extract top elements from our RDD using top(). Action top() use default ordering of data.\n",
    "\n",
    "Top() example:\n",
    "\n",
    "[php]val data = spark.read.textFile(“spark_test.txt”).rdd\n",
    "val mapFile = data.map(line => (line,line.length))\n",
    "val res = mapFile.top(3)\n",
    "res.foreach(println)[/php]\n",
    "\n",
    "Note – map() operation will map each line with its length. And top(3) will return 3 records from mapFile with default ordering.\n",
    "4.5. countByValue()\n",
    "The countByValue() returns, many times each element occur in RDD.\n",
    "\n",
    "For example, RDD has values {1, 2, 2, 3, 4, 5, 5, 6} in this RDD “rdd.countByValue()”  will give the result {(1,1), (2,2), (3,1), (4,1), (5,2), (6,1)}\n",
    "\n",
    "countByValue() example:\n",
    "\n",
    "[php]val data = spark.read.textFile(“spark_test.txt”).rdd\n",
    "val result= data.map(line => (line,line.length)).countByValue()\n",
    "result.foreach(println)[/php]\n",
    "\n",
    "Note – The countByValue() action will return a hashmap of (K, Int) pairs with the count of each key.\n",
    "Learn: Apache Spark Streaming Transformation Operations\n",
    "\n",
    "4.6. reduce()\n",
    "The reduce() function takes the two elements as input from the RDD and then produces the output of the same type as that of the input elements. The simple forms of such function are an addition. We can add the elements of RDD, count the number of words. It accepts commutative and associative operations as an argument.\n",
    "\n",
    "Reduce() example:\n",
    "\n",
    "[php]val rdd1 = spark.sparkContext.parallelize(List(20,32,45,62,8,5))\n",
    "val sum = rdd1.reduce(_+_)\n",
    "println(sum)[/php]\n",
    "\n",
    "Note – The reduce() action in above code will add the elements of the source RDD.\n",
    "4.7. fold()\n",
    "The signature of the fold() is like reduce(). Besides, it takes “zero value” as input, which is used for the initial call on each partition. But, the condition with zero value is that it should be the identity element of that operation. The key difference between fold() and reduce() is that, reduce() throws an exception for empty collection, but fold() is defined for empty collection.\n",
    "\n",
    "For example, zero is an identity for addition; one is identity element for multiplication. The return type of fold() is same as that of the element of RDD we are operating on.\n",
    "For example, rdd.fold(0)((x, y) => x + y).\n",
    "\n",
    "Fold() example:\n",
    "\n",
    "[php]val rdd1 = spark.sparkContext.parallelize(List((“maths”, 80),(“science”, 90)))\n",
    "val additionalMarks = (“extra”, 4)\n",
    "val sum = rdd1.fold(additionalMarks){ (acc, marks) => val add = acc._2 + marks._2\n",
    "(“total”, add)\n",
    "}\n",
    "println(sum)[/php]\n",
    "\n",
    "Note – In above code additionalMarks is an initial value. This value will be added to the int value of each record in the source RDD.\n",
    "Learn: Spark Streaming Checkpoint in Apache Spark\n",
    "\n",
    "4.8. aggregate()\n",
    "It gives us the flexibility to get data type different from the input type. The aggregate() takes two functions to get the final result. Through one function we combine the element from our RDD with the accumulator, and the second, to combine the accumulator. Hence, in aggregate, we supply the initial zero value of the type which we want to return.\n",
    "\n",
    "4.9. foreach()\n",
    "When we have a situation where we want to apply operation on each element of RDD, but it should not return value to the driver. In this case, foreach() function is useful. For example, inserting a record into the database.\n",
    "\n",
    "Foreach() example:\n",
    "\n",
    "[php]val data = spark.sparkContext.parallelize(Array((‘k’,5),(‘s’,3),(‘s’,4),(‘p’,7),(‘p’,5),(‘t’,8),(‘k’,6)),3)\n",
    "val group = data.groupByKey().collect()\n",
    "group.foreach(println)[/php]\n",
    "\n",
    "Note – The foreach() action run a function (println) on each element of the dataset group.\n",
    "5. Conclusion\n",
    "In conclusion, on applying a transformation to an RDD creates another RDD. As a result of this RDDs are immutable in nature. \n",
    "On the introduction of an action on an RDD, the result gets computed. \n",
    "Thus, this lazy evaluation decreases the overhead of computation and make the system more efficient.\n",
    "                                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate.\n",
    "Solution:-  Map()-> The map transformation is the most commonly used and the simplest of transformations on an RDD. The map transformation applies the function passed in the arguments to each of the elements of the source RDD. In the previous examples, \n",
    "we have seen the usage of map() transformation where we have passed the split() function to the input RDD.\n",
    "\n",
    "Example:- \n",
    "val data = Seq(\"Project Gutenberg’s\",\n",
    "    \"Alice’s Adventures in Wonderland\",\n",
    "    \"Project Gutenberg’s\",\n",
    "    \"Adventures in Wonderland\",\n",
    "    \"Project Gutenberg’s\")\n",
    "\n",
    "import spark.sqlContext.implicits._\n",
    "val df = data.toDF(\"data\")\n",
    "df.show()\n",
    "\n",
    "//Output\n",
    "+--------------------+\n",
    "|                data|\n",
    "+--------------------+\n",
    "| Project Gutenberg’s|\n",
    "|Alice’s Adventure...|\n",
    "| Project Gutenberg’s|\n",
    "|Adventures in Won...|\n",
    "| Project Gutenberg’s|\n",
    "+--------------------+\n",
    "\n",
    "\n",
    "//Map Transformation\n",
    "val mapDF=df.map(fun=> {\n",
    "    fun.getString(0).split(\" \")\n",
    "})\n",
    "mapDF.show()\n",
    "\n",
    "//Output\n",
    "+-------------------------------------+\n",
    "|value                                |\n",
    "+-------------------------------------+\n",
    "|[Project, Gutenberg’s]               |\n",
    "|[Alice’s, Adventures, in, Wonderland]|\n",
    "|[Project, Gutenberg’s]               |\n",
    "|[Adventures, in, Wonderland]         |\n",
    "|[Project, Gutenberg’s]               |\n",
    "+-------------------------------------+\n",
    "\n",
    "Filter()-> Filter, as the name implies, filters the input RDD, and creates a new dataset that satisfies the predicate passed as arguments.\n",
    "\n",
    "Spark RDD filter() function returns a new RDD, containing only the elements that meet a predicate. It is a narrow operation because it does not shuffle data from one partition to many partitions.\n",
    "\n",
    "For example, Suppose RDD contains first five natural numbers (1, 2, 3, 4, and 5) and the predicate is check for an even number. The resulting RDD after the filter will contain only the even numbers i.e., 2 and 4.\n",
    "\n",
    "Filter() example:\n",
    "\n",
    "[php]val data = spark.read.textFile(“spark_test.txt”).rdd\n",
    "val mapFile = data.flatMap(lines => lines.split(” “)).filter(value => value==”spark”)\n",
    "println(mapFile.count())[/php]\n",
    "\n",
    "Note – In above code, flatMap function map line into words and then count the word “Spark” using count() Action after filtering lines containing “Spark” from mapFile.\n",
    "\n",
    "Reduce()\n",
    "The reduce() function takes the two elements as input from the RDD and then produces the output of the same type as that of the input elements. The simple forms of such function are an addition. We can add the elements of RDD, count the number of words. It accepts commutative and associative operations as an argument.\n",
    "\n",
    "Reduce() example:\n",
    "\n",
    "[php]val rdd1 = spark.sparkContext.parallelize(List(20,32,45,62,8,5))\n",
    "val sum = rdd1.reduce(_+_)\n",
    "println(sum)[/php]\n",
    "\n",
    "\n",
    "Aggregate()\n",
    "It gives us the flexibility to get data type different from the input type. The aggregate() takes two functions to get the final result. Through one function we combine the element from our RDD with the accumulator, and the second, to combine the accumulator. \n",
    "Hence, in aggregate, we supply the initial zero value of the type which we want to return.\n",
    "\n",
    "Example:-  val listRdd = spark.sparkContext.parallelize(List(1,2,3,4,5,3,2))\n",
    "  def param0= (accu:Int, v:Int) => accu + v\n",
    "  def param1= (accu1:Int,accu2:Int) => accu1 + accu2\n",
    "  val result = listRdd.aggregate(0)(param0,param1)\n",
    "  println(\"output 1 =>\" + result)\n",
    "\n",
    "In this snippet, RDD type is Int and it returns the result of Int type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd67021",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 2. Spark DataFrame Operations:\n",
    "a) Write a Python program to load a CSV file into a Spark DataFrame.\n",
    "\n",
    "Solution:- Loading a simple CSV to Dataframe is very easy in Spark. But it gets messy when raw data has new line characters in between.\n",
    "\n",
    "Take a look at the sample data. The first row has an additional newline character after the word “Rachel green”.\n",
    "\n",
    "id,name,description,status\n",
    "1,rachel,\"rachel green \n",
    "started her career at central perk\",true\n",
    "2,joey,\"joey tribainni's fav line is, how you doing?\",true\n",
    "When loaded to a data frame it looks like this\n",
    "\n",
    "%python\n",
    "file_location = \"/FileStore/tables/multilinetext_csv.bz2\"\n",
    "\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "display(df)\n",
    "\n",
    "Once the problem is identified, the fix is very simple. Add one more parameter to “multiLine”, “true”.\n",
    "\n",
    "Mentioning the quote character is purely optional.\n",
    "\n",
    "%python\n",
    "\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .option(\"multiLine\", \"true\") \\\n",
    "  .option(\"quote\",\"\\\"\") \\\n",
    "  .load(file_location)\n",
    "display(df)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57afb5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    " b)Perform common DataFrame operations such as filtering, grouping, or joining.\n",
    "    \n",
    "Solution:-  Spark DataFrame Operations\n",
    "Some of the basic and frequently used spark dataframe operations would be discussed below.\n",
    "\n",
    "Before we start, let’s create our SparkSession and sparkContext. (Note: These parameters are automatically created if you’re accessing spark via spark shell)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Spark Training').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "Create Spark DataFrame\n",
    "Below are some of the methods to create a pyspark dataframe.\n",
    "\n",
    "Creating Spark Dataframe from CSV File using spark.read.csv method.\n",
    "For this example, a countrywise population by year dataset is chosen. The dataset can be downloaded here, population_dataset\n",
    "\n",
    "df = spark.read.format('csv').options(delimiter=',', header=True).load('/Path-to-file/population.csv')\n",
    "Convert RDD to Dataframe.\n",
    "Below method shows how to create DataFrame from RDD. The toDF() method can be used to convert the RDD to a dataframe\n",
    "\n",
    "rdd = sc.parallelize([(1,2),(3,4),(5,6)])\n",
    "rdf = rdd.toDF()\n",
    "Using spark.createDataFrame method.\n",
    "Creating a DataFrame from a list of values. Schema is inferred dynamically, if not specified.\n",
    "\n",
    "tdf = spark.createDataFrame([('Alice',24),('David',43)],['name','age'])\n",
    "tdf.printSchema()\n",
    "root\n",
    " |-- name: string (nullable = true)\n",
    " |-- age: long (nullable = true)\n",
    "\n",
    "\n",
    "\n",
    "Spark DataFrame Schema\n",
    "The df.printSchema() method can be used to display the schema of spark dataframe\n",
    "\n",
    "df.printSchema()\n",
    "root\n",
    " |-- Country Name: string (nullable = true)\n",
    " |-- Country Code: string (nullable = true)\n",
    " |-- Year: string (nullable = true)\n",
    " |-- Value: string (nullable = true)\n",
    "To obtain the raw schema of a dataframe, the df.schema method can be used.\n",
    "\n",
    "df.schema\n",
    "StructType(List(StructField(Country Name,StringType,true),StructField(Country Code,StringType,true),StructField(Year,LongType,true),StructField(Value,LongType,true)))\n",
    "To display the columns of dataframe, df.columns method can be used. A list consisting of the columns is generated.\n",
    "\n",
    "df.columns\n",
    "['Country Name', 'Country Code', 'Year', 'Value']\n",
    "\n",
    "\n",
    "\n",
    "Count of a Spark DataFrame\n",
    "df.count()\n",
    "14885\n",
    "\n",
    "\n",
    "\n",
    "Display DataFrame Data\n",
    "Display 5 rows. Truncate=False can be enabled for displaying entire column data on your terminal\n",
    "\n",
    "df.show(5, truncate=False)\n",
    "+------------+------------+----+---------+\n",
    "|Country Name|Country Code|Year|Value    |\n",
    "+------------+------------+----+---------+\n",
    "|Arab World  |ARB         |1960|92490932 |\n",
    "|Arab World  |ARB         |1961|95044497 |\n",
    "|Arab World  |ARB         |1962|97682294 |\n",
    "|Arab World  |ARB         |1963|100411076|\n",
    "|Arab World  |ARB         |1964|103239902|\n",
    "+------------+------------+----+---------+\n",
    "only showing top 5 rows\n",
    "\n",
    "\n",
    "\n",
    "Remove Duplicate rows from a DataFrame\n",
    "df.dropDuplicates() can be used to remove duplicates from a spark dataframe.\n",
    "\n",
    "df.dropDuplicates()\n",
    "DataFrame[Country Name: string, Country Code: string, Year: bigint, Value: bigint]\n",
    "\n",
    "\n",
    "\n",
    "Distinct Column Values\n",
    "To display distinct rows of a dataframe, df.distinct() can be used. For our example let’s select distinct country code from the dataset\n",
    "\n",
    "df.select('Country Code').distinct().show()\n",
    "+------------+\n",
    "|Country Code|\n",
    "+------------+\n",
    "|         HTI|\n",
    "|         PSE|\n",
    "|         LTE|\n",
    "|         BRB|\n",
    "|         LVA|\n",
    "|         POL|\n",
    "|         ECS|\n",
    "|         TEA|\n",
    "|         JAM|\n",
    "|         ZMB|\n",
    "|         MIC|\n",
    "|         BRA|\n",
    "|         ARM|\n",
    "|         IDA|\n",
    "|         MOZ|\n",
    "|         CUB|\n",
    "|         JOR|\n",
    "|         OSS|\n",
    "|         ABW|\n",
    "|         FRA|\n",
    "+------------+\n",
    "only showing top 20 rows\n",
    "\n",
    "\n",
    "\n",
    "Spark Filter Data\n",
    "df.filter() method can be used to filter in pyspark. In our example, let’s display population of India for the year 2015,2016 & - and\n",
    "\n",
    "| - or\n",
    "\n",
    "Note: Remember to wrap the conditions with braces when ‘&’ or ‘|’ is used.\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df.filter((col('Country Name') == 'India') & (col('Year').isin('2015','2016'))).show()\n",
    "+------------+------------+----+----------+\n",
    "|Country Name|Country Code|Year|     Value|\n",
    "+------------+------------+----+----------+\n",
    "|       India|         IND|2015|1309053980|\n",
    "|       India|         IND|2016|1324171354|\n",
    "+------------+------------+----+----------+\n",
    "\n",
    "\n",
    "\n",
    "Sorting/Ordering Data in Spark\n",
    "Below example illustrates how the country names can be displayed in descending order\n",
    "\n",
    "df.select('Country Name').orderBy('Country Name', ascending=False).distinct().show(truncate=False)\n",
    "+------------------------+\n",
    "|Country Name            |\n",
    "+------------------------+\n",
    "|Zimbabwe                |\n",
    "|Zambia                  |\n",
    "|Yemen, Rep.             |\n",
    "|World                   |\n",
    "|West Bank and Gaza      |\n",
    "|Virgin Islands (U.S.)   |\n",
    "|Vietnam                 |\n",
    "|Venezuela, RB           |\n",
    "|Vanuatu                 |\n",
    "|Uzbekistan              |\n",
    "|Uruguay                 |\n",
    "|Upper middle income     |\n",
    "|United States           |\n",
    "|United Kingdom          |\n",
    "|United Arab Emirates    |\n",
    "|Ukraine                 |\n",
    "|Uganda                  |\n",
    "|Tuvalu                  |\n",
    "|Turks and Caicos Islands|\n",
    "|Turkmenistan            |\n",
    "+------------------------+\n",
    "only showing top 20 rows\n",
    "\n",
    "\n",
    "\n",
    "Grouping & Performing Aggregations in a Spark Dataframe\n",
    "Obtain the total count of distinct years present in the entire dataset\n",
    "# Count of Years\n",
    "df.select('Year').distinct().groupBy().count().show()\n",
    "+-----+\n",
    "|count|\n",
    "+-----+\n",
    "|   57|\n",
    "+-----+\n",
    "\n",
    "\n",
    "Calculate sum of dataframe – Compute the total world population for the year 1990\n",
    "df.filter(col('Year') == '1990').agg({'Value':'sum'}).show(truncate=False)\n",
    "+---------------+\n",
    "|sum(Value)     |\n",
    "+---------------+\n",
    "|5.4935613753E10|\n",
    "+---------------+\n",
    "\n",
    "Calculate average of dataframe – Compute the average population in India for the year 2005\n",
    "df.filter((col('Year') == '2005') & (col('Country Name') == 'India')).agg({'Value':'avg'}).show(truncate=False)\n",
    "+-------------+\n",
    "|avg(Value)   |\n",
    "+-------------+\n",
    "|1.144118674E9|\n",
    "+-------------+\n",
    "\n",
    "Computing Minimum of a column in dataframe – Display the least population for the year 2010\n",
    "df.filter(df.Year == '2007').agg({'Value':'min'}).show()\n",
    "+----------+\n",
    "|min(Value)|\n",
    "+----------+\n",
    "|     10075|\n",
    "+----------+\n",
    "\n",
    "Computing Maximum of a column in dataframe – Display country with the largest population for the year 2016\n",
    "df.filter(df.Value == df.filter(df.Year == '2016').agg({'Value':'max'}).collect()[0][0]).show()\n",
    "+------------+------------+----+----------+\n",
    "|Country Name|Country Code|Year|     Value|\n",
    "+------------+------------+----+----------+\n",
    "|       World|         WLD|2016|7442135578|\n",
    "+------------+------------+----+----------+\n",
    "\n",
    "\n",
    "\n",
    "Spark Join DataFrames\n",
    "A pyspark dataframe can be joined with another using the df.join method. df.join takes 3 arguments, join(other, on=None, how=None)\n",
    "other - dataframe to be joined with\n",
    "on - on condition of the join\n",
    "how - type of join. inner join is set by default if not specified\n",
    "Other types of joins which can be specified are, inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and left_anti\n",
    "\n",
    "Below is an example illustrating an inner join in pyspark\n",
    "Let’s construct 2 dataframes,\n",
    "One with only distinct values of country name and country code and the other with country code, value and year\n",
    "Country code would be the join condition here\n",
    "\n",
    "df1 = df.select('Country Name', 'Country Code').distinct()\n",
    "df1.count()\n",
    "263\n",
    "df2 = df.select(col('Country Code').alias('ctry_cd'), 'Value', 'Year').distinct()\n",
    "df2.count()\n",
    "14885\n",
    "\n",
    "Now let’s join both the dataframes on country_code and display the data\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df1.join(df2, col('Country Code') == col('ctry_cd')).show(5)\n",
    "+--------------------+------------+-------+----------+----+\n",
    "|        Country Name|Country Code|ctry_cd|     Value|Year|\n",
    "+--------------------+------------+-------+----------+----+\n",
    "|East Asia & Pacif...|         EAP|    EAP|1878255588|2004|\n",
    "|Europe & Central ...|         ECA|    ECA| 396886165|2001|\n",
    "|           IDA blend|         IDB|    IDB| 135810058|1964|\n",
    "|           IDA blend|         IDB|    IDB| 403526930|2005|\n",
    "|            IDA only|         IDX|    IDX| 984961696|2013|\n",
    "+--------------------+------------+-------+----------+----+\n",
    "only showing top 5 rows\n",
    "\n",
    "Country Code seems to be redundant here, so while displaying this can be removed using the drop method\n",
    "\n",
    "df1.join(df2, col('Country Code') == col('ctry_cd')).drop(col('ctry_cd')).show(5,False)\n",
    "+---------------------------------------------+------------+----------+----+\n",
    "|Country Name                                 |Country Code|Value     |Year|\n",
    "+---------------------------------------------+------------+----------+----+\n",
    "|East Asia & Pacific (excluding high income)  |EAP         |1878255588|2004|\n",
    "|Europe & Central Asia (excluding high income)|ECA         |396886165 |2001|\n",
    "|IDA blend                                    |IDB         |135810058 |1964|\n",
    "|IDA blend                                    |IDB         |403526930 |2005|\n",
    "|IDA only                                     |IDX         |984961696 |2013|\n",
    "+---------------------------------------------+------------+----------+----+\n",
    "only showing top 5 rows\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f183d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "c) Apply Spark SQL queries on the DataFrame to extract insights from the data.\n",
    "\n",
    "Solution:-  Spark SQL seamlessly integrates with the Spark ecosystem, providing an interface to query structured data using SQL queries. It leverages the powerful distributed computing capabilities of Spark, allowing for efficient and scalable data processing. Spark SQL extends the concept of DataFrames, offering a high-level abstraction for working with structured data.\n",
    "\n",
    "Loading and Exploring Data\n",
    "\n",
    "To demonstrate the capabilities of Spark SQL, let’s consider a scenario where we have a dataset containing information about online product reviews. The dataset includes attributes such as reviewer ID, product ID, rating, and review text. We will load this dataset and perform various exploratory tasks using Spark SQL.\n",
    "\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = spark.read.csv(\"path/to/reviews.csv\", header=True, inferSchema=True)\n",
    "Once the dataset is loaded into a DataFrame, we can explore its structure and contents. Use the following methods to gain insights:\n",
    "\n",
    "#To view the schema of the DataFrame:\n",
    "df.printSchema()\n",
    "#To display the first few rows of the DataFrame:\n",
    "df.show()\n",
    "#To get summary statistics of numerical columns:\n",
    "df.describe().show()\n",
    "#To count the number of rows in the DataFrame:\n",
    "print(\"Number of rows:\", df.count())\n",
    "These operations will help us understand the structure and content of the dataset, allowing us to formulate meaningful queries and gain insights.\n",
    "\n",
    "Querying Data using SQL\n",
    "\n",
    "One of the major strengths of Spark SQL is the ability to execute SQL queries on DataFrames. Let’s explore some interesting queries we can perform on our dataset:\n",
    "\n",
    "Task 1: Top Reviewers\n",
    "We can identify the top reviewers based on the number of reviews they have submitted. This can be achieved with the following SQL query:\n",
    "\n",
    "df.createOrReplaceTempView(\"reviews\")\n",
    "query = \"SELECT reviewerID, COUNT(*) as reviewCount FROM reviews GROUP BY reviewerID ORDER BY reviewCount DESC LIMIT 10\"\n",
    "top_reviewers = spark.sql(query)\n",
    "This query counts the number of reviews submitted by each reviewer and retrieves the top 10 reviewers based on the review count.\n",
    "\n",
    "Task 2: Sentiment Analysis\n",
    "We can perform sentiment analysis on the review text to understand the overall sentiment expressed in the reviews. To accomplish this, we can utilize Spark SQL’s built-in functions and user-defined functions (UDFs):\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define a UDF to perform sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    " # Add your sentiment analysis logic here\n",
    " return \"positive\" if text.count(\"good\") > text.count(\"bad\") else \"negative\"\n",
    "\n",
    "# Register the UDF\n",
    "sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "spark.udf.register(\"analyze_sentiment\", sentiment_udf)\n",
    "\n",
    "# Apply sentiment analysis on the review text\n",
    "df = df.withColumn(\"sentiment\", analyze_sentiment(df[\"reviewText\"]))\n",
    "In this example, we define a UDF called `analyze_sentiment` to perform sentiment analysis on the review text. We register the UDF with Spark SQL and apply it to the DataFrame, adding a new column called “sentiment” that represents the sentiment of each review.\n",
    "\n",
    "Task 3: Average Ratings by Product Category\n",
    "We can calculate the average ratings for each product category to identify the highest-rated categories. This can be achieved with the following SQL query:\n",
    "\n",
    "query = \"SELECT category, AVG(overall) as averageRating FROM reviews GROUP BY category ORDER BY averageRating DESC\"\n",
    "average_ratings = spark.sql(query)\n",
    "This query calculates the average rating for each product category and orders the results by the average rating in descending order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebdf573",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 3. Spark Streaming:\n",
    "a) Write a Python program to create a Spark Streaming application.\n",
    "\n",
    "Solution:- Sample Python Spark Streaming application:\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import*\n",
    "from pyspark.sql.types import*\n",
    "import time\n",
    "\n",
    "#create spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Connect to kafka server and read data stream\n",
    "df = spark \\\n",
    ".readStream \\\n",
    ".format(\"kafka\") \\\n",
    ".option(\"kafka.bootstrap.servers\", \"CHANGEME_KAFKA_SERVER\") \\\n",
    ".option(\"kafka.sasl.jaas.config\",\"org.apache.kafka.common.security.plain.PlainLoginModule required username='CHANGEME_USERNAME' password='CHANGEME_PASSWORD';\") \\\n",
    ".option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
    ".option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    ".option(\"kafka.ssl.protocol\", \"TLSv1.2\") \\\n",
    ".option(\"kafka.ssl.enabled.protocols\", \"TLSv1.2\") \\\n",
    ".option(\"kafka.ssl.endpoint.identification.algorithm\", \"HTTPS\") \\\n",
    ".option(\"subscribe\", \"CHANGEME_TOPIC\") \\\n",
    ".load() \\\n",
    ".selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# Write the input data to memory\n",
    "query = df.writeStream.outputMode(\"append\").format(\"memory\").queryName(\"testk2s\").option(\"partition.assignment.strategy\", \"range\").start()\n",
    "\n",
    "query.awaitTermination(30)\n",
    "\n",
    "query.stop()\n",
    "\n",
    "query.status\n",
    "\n",
    "# Query data\n",
    "test_result=spark.sql(\"select * from testk2s\")\n",
    "test_result.show(5)\n",
    "\n",
    "spark.sql(\"select count(*) from testk2s\").show()\n",
    "test_result_small = spark.sql(\"select * from testk2s limit 5\")\n",
    "test_result_small.show()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket).\n",
    "  \n",
    "Solution:- Solution Approach\n",
    "1. Create a SparkSession object.\n",
    "\n",
    "2. Set up a Spark Streaming context with the appropriate configurations.\n",
    "\n",
    "3. Define Kafka configuration properties, including the Kafka bootstrap servers, topic name, and any additional producer properties.\n",
    "\n",
    "\n",
    "4. Create a DStream that represents the data stream to be processed (e.g., from a socket, Kafka source, or other streaming source).\n",
    "\n",
    "5. Apply transformations and processing operations on the DStream to derive insights or perform calculations.\n",
    "\n",
    "\n",
    "6. Use the Kafka producer API to write the processed data to a Kafka topic.\n",
    "\n",
    "Code\n",
    "\n",
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"KafkaStreamingExample\").getOrCreate()\n",
    "# Set the batch interval for Spark Streaming (e.g., 1 second)\n",
    "batch_interval = 1\n",
    "\n",
    "# Create a Spark Streaming context\n",
    "ssc = StreamingContext(spark.sparkContext, batch_interval)\n",
    "\n",
    "# Define Kafka configuration properties\n",
    "kafka_bootstrap_servers = \"<kafka_bootstrap_servers>\"\n",
    "kafka_topic = \"<kafka_topic>\"\n",
    "producer_properties = {\n",
    "    \"bootstrap.servers\": kafka_bootstrap_servers\n",
    "}\n",
    "\n",
    "# Create a DStream that represents the data stream to be processed\n",
    "data_stream = ssc.socketTextStream(\"<hostname>\", <port>)\n",
    "\n",
    "# Apply transformations and processing operations on the data stream\n",
    "processed_stream = data_stream.flatMap(lambda line: line.split(\" \")) \\\n",
    "                             .filter(lambda word: len(word) > 0)\n",
    "\n",
    "# Write the processed data to a Kafka topic using the Kafka producer API\n",
    "processed_stream.foreachRDD(lambda rdd: rdd.foreachPartition(write_to_kafka))\n",
    "\n",
    "# Kafka producer function to write data to Kafka\n",
    "def write_to_kafka(partition):\n",
    "    producer = KafkaProducer(**producer_properties)\n",
    "    for record in partition:\n",
    "        producer.send(kafka_topic, value=record.encode(\"utf-8\"))\n",
    "    producer.close()\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "\n",
    "# Await termination or stop the streaming context manually\n",
    "ssc.awaitTermination()\n",
    "Explanation\n",
    "– First, we import the necessary libraries, including SparkSession, StreamingContext, KafkaUtils, and KafkaProducer, to work with Spark Streaming and Kafka.\n",
    "\n",
    "– We create a SparkSession object to provide a single entry point for Spark functionality.\n",
    "\n",
    "\n",
    "– Next, we set the batch interval for the streaming context, which defines how often the streaming data is processed (e.g., 1 second).\n",
    "\n",
    "– We create a StreamingContext object by passing the SparkContext and batch interval as parameters.\n",
    "\n",
    "– We define the Kafka configuration properties, including the bootstrap servers.\n",
    "\n",
    "\n",
    "– Using the appropriate streaming source (e.g., socketTextStream), we create a DStream that represents the data stream to be processed.\n",
    "\n",
    "– We apply transformations on the DStream, such as splitting each line into words and filtering out empty words.\n",
    "\n",
    "– Finally, we use the foreachRDD function to write the processed data to Kafka. The write_to_kafka function is defined to handle writing data to Kafka within each RDD partition.\n",
    "\n",
    " \n",
    "Key Considerations:\n",
    "\n",
    "– Ensure that the Spark Streaming and Kafka dependencies are correctly configured and available in your environment.\n",
    "\n",
    "– Provide the appropriate Kafka bootstrap servers, topic name, and any additional producer properties.\n",
    "\n",
    "\n",
    "– Consider scalability and resource allocation to handle increasing data volumes and processing requirements.\n",
    "\n",
    "– Handle exceptions and ensure fault tolerance in case of failures or connectivity issues with Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f724b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c) Implement streaming transformations and actions to process and analyze the incoming data stream.\n",
    "\n",
    "Solution:- Spark Transformation is a function that produces new RDD from the existing RDDs. It takes RDD as input and produces one or more RDD as output. Each time it creates new RDD when we apply any transformation. Thus, the so input RDDs, cannot be changed since RDD are immutable in nature.\n",
    "\n",
    "Applying transformation built an RDD lineage, with the entire parent RDDs of the final RDD(s). RDD lineage, also known as RDD operator graph or RDD dependency graph. It is a logical execution plan i.e., it is Directed Acyclic Graph (DAG) of the entire parent RDDs of RDD.\n",
    "\n",
    "Transformations are lazy in nature i.e., they get execute when we call an action. They are not executed immediately. Two most basic type of transformations is a map(), filter().\n",
    "After the transformation, the resultant RDD is always different from its parent RDD. It can be smaller (e.g. filter, count, distinct, sample), bigger (e.g. flatMap(), union(), Cartesian()) or the same size (e.g. map).\n",
    "\n",
    "There are two types of transformations:\n",
    "\n",
    "Narrow transformation – In Narrow transformation, all the elements that are required to compute the records in single partition live in the single partition of parent RDD. A limited subset of partition is used to calculate the result. Narrow transformations are the result of map(), filter().\n",
    "\n",
    "Wide transformation – In wide transformation, all the elements that are required to compute the records in the single partition may live in many partitions of parent RDD. The partition may live in many partitions of parent RDD. Wide transformations are the result of groupbyKey() and reducebyKey().\n",
    "\n",
    "There are various functions in RDD transformation. Let us see RDD transformation with examples.\n",
    "\n",
    "3.1. map(func)\n",
    "The map function iterates over every line in RDD and split into new RDD. Using map() transformation we take in any function, and that function is applied to every element of RDD.\n",
    "\n",
    "In the map, we have the flexibility that the input and the return type of RDD may differ from each other. For example, we can have input RDD type as String, after applying the\n",
    "\n",
    "map() function the return RDD can be Boolean.\n",
    "\n",
    "For example, in RDD {1, 2, 3, 4, 5} if we apply “rdd.map(x=>x+2)” we will get the result as (3, 4, 5, 6, 7).\n",
    "\n",
    "Also Read: How to create RDD\n",
    "\n",
    "Map() example:\n",
    "\n",
    "[php]import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "object  mapTest{\n",
    "def main(args: Array[String]) = {\n",
    "val spark = SparkSession.builder.appName(“mapExample”).master(“local”).getOrCreate()\n",
    "val data = spark.read.textFile(“spark_test.txt”).rdd\n",
    "val mapFile = data.map(line => (line,line.length))\n",
    "mapFile.foreach(println)\n",
    "}\n",
    "}[/php]\n",
    "\n",
    "spark_test.txt”\n",
    "\n",
    "hello...user! this file is created to check the operations of spark.\n",
    "?, and how can we apply functions on that RDD partitions?. All this will be done through spark programming which is done with the help of scala language support…\n",
    "Note – In above code, map() function map each line of the file with its length.\n",
    "3.2. flatMap()\n",
    "With the help of flatMap() function, to each input element, we have many elements in an output RDD. The most simple use of flatMap() is to split each input string into words.\n",
    "Map and flatMap are similar in the way that they take a line from input RDD and apply a function on that line. The key difference between map() and flatMap() is map() returns only one element, while flatMap() can return a list of elements.\n",
    "\n",
    "flatMap() example:\n",
    "\n",
    "[php]val data = spark.read.textFile(“spark_test.txt”).rdd\n",
    "val flatmapFile = data.flatMap(lines => lines.split(” “))\n",
    "flatmapFile.foreach(println)[/php]\n",
    "\n",
    "Note – In above code, flatMap() function splits each line when space occurs.\n",
    "3.3. filter(func)\n",
    "Spark RDD filter() function returns a new RDD, containing only the elements that meet a predicate. It is a narrow operation because it does not shuffle data from one partition to many partitions.\n",
    "\n",
    "For example, Suppose RDD contains first five natural numbers (1, 2, 3, 4, and 5) and the predicate is check for an even number. The resulting RDD after the filter will contain only the even numbers i.e., 2 and 4.\n",
    "\n",
    "Filter() example:\n",
    "\n",
    "[php]val data = spark.read.textFile(“spark_test.txt”).rdd\n",
    "val mapFile = data.flatMap(lines => lines.split(” “)).filter(value => value==”spark”)\n",
    "println(mapFile.count())[/php]\n",
    "\n",
    "Note – In above code, flatMap function map line into words and then count the word “Spark” using count() Action after filtering lines containing “Spark” from mapFile.\n",
    "Read: Apache Spark RDD vs DataFrame vs DataSet\n",
    "\n",
    "3.4. mapPartitions(func)\n",
    "The MapPartition converts each partition of the source RDD into many elements of the result (possibly none). In mapPartition(), the map() function is applied on each partitions simultaneously. MapPartition is like a map, but the difference is it runs separately on each partition(block) of the RDD.\n",
    "\n",
    "3.5. mapPartitionWithIndex()\n",
    "It is like mapPartition; Besides mapPartition it provides func with an integer value representing the index of the partition, and the map() is applied on partition index wise one after the other.\n",
    "\n",
    "Learn: Spark Shell Commands to Interact with Spark-Scala\n",
    "\n",
    "3.6. union(dataset)\n",
    "With the union() function, we get the elements of both the RDD in new RDD. The key rule of this function is that the two RDDs should be of the same type.\n",
    "For example, the elements of RDD1 are (Spark, Spark, Hadoop, Flink) and that of RDD2 are (Big data, Spark, Flink) so the resultant rdd1.union(rdd2) will have elements (Spark, Spark, Spark, Hadoop, Flink, Flink, Big data).\n",
    "\n",
    "Union() example:\n",
    "\n",
    "[php]val rdd1 = spark.sparkContext.parallelize(Seq((1,”jan”,2016),(3,”nov”,2014),(16,”feb”,2014)))\n",
    "val rdd2 = spark.sparkContext.parallelize(Seq((5,”dec”,2014),(17,”sep”,2015)))\n",
    "val rdd3 = spark.sparkContext.parallelize(Seq((6,”dec”,2011),(16,”may”,2015)))\n",
    "val rddUnion = rdd1.union(rdd2).union(rdd3)\n",
    "rddUnion.foreach(Println)[/php]\n",
    "\n",
    "Note – In above code union() operation will return a new dataset that contains the union of the elements in the source dataset (rdd1) and the argument (rdd2 & rdd3).\n",
    "3.7. intersection(other-dataset)\n",
    "With the intersection() function, we get only the common element of both the RDD in new RDD. The key rule of this function is that the two RDDs should be of the same type.\n",
    "Consider an example, the elements of RDD1 are (Spark, Spark, Hadoop, Flink) and that of RDD2 are (Big data, Spark, Flink) so the resultant rdd1.intersection(rdd2) will have elements (spark).\n",
    "\n",
    "Intersection() example:\n",
    "\n",
    "[php]val rdd1 = spark.sparkContext.parallelize(Seq((1,”jan”,2016),(3,”nov”,2014, (16,”feb”,2014)))\n",
    "val rdd2 = spark.sparkContext.parallelize(Seq((5,”dec”,2014),(1,”jan”,2016)))\n",
    "val comman = rdd1.intersection(rdd2)\n",
    "comman.foreach(Println)[/php]\n",
    "\n",
    "Note – The intersection() operation return a new RDD. It contains the intersection of elements in the rdd1 & rdd2.\n",
    "Learn to Install Spark on Ubuntu\n",
    "\n",
    "3.8. distinct()\n",
    "It returns a new dataset that contains the distinct elements of the source dataset. It is helpful to remove duplicate data.\n",
    "For example, if RDD has elements (Spark, Spark, Hadoop, Flink), then rdd.distinct() will give elements (Spark, Hadoop, Flink).\n",
    "\n",
    "Distinct() example:\n",
    "\n",
    "[php]val rdd1 = park.sparkContext.parallelize(Seq((1,”jan”,2016),(3,”nov”,2014),(16,”feb”,2014),(3,”nov”,2014)))\n",
    "val result = rdd1.distinct()\n",
    "println(result.collect().mkString(“, “))[/php]\n",
    "\n",
    "Note – In the above example, the distinct function will remove the duplicate record i.e. (3,'”nov”,2014).\n",
    "3.9. groupByKey()\n",
    "When we use groupByKey() on a dataset of (K, V) pairs, the data is shuffled according to the key value K in another RDD. In this transformation, lots of unnecessary data get to transfer over the network.\n",
    "\n",
    "Spark provides the provision to save data to disk when there is more data shuffled onto a single executor machine than can fit in memory. Follow this link to learn about RDD Caching and Persistence mechanism in detail.\n",
    "\n",
    "groupByKey() example:\n",
    "\n",
    "[php]val data = spark.sparkContext.parallelize(Array((‘k’,5),(‘s’,3),(‘s’,4),(‘p’,7),(‘p’,5),(‘t’,8),(‘k’,6)),3)\n",
    "val group = data.groupByKey().collect()\n",
    "group.foreach(println)[/php]\n",
    "\n",
    "Note – The groupByKey() will group the integers on the basis of same key(alphabet). After that collect() action will return all the elements of the dataset as an Array.\n",
    "3.10. reduceByKey(func, [numTasks])\n",
    "When we use reduceByKey on a dataset (K, V), the pairs on the same machine with the same key are combined, before the data is shuffled.\n",
    "\n",
    "reduceByKey() example:\n",
    "\n",
    "[php]val words = Array(“one”,”two”,”two”,”four”,”five”,”six”,”six”,”eight”,”nine”,”ten”)\n",
    "val data = spark.sparkContext.parallelize(words).map(w => (w,1)).reduceByKey(_+_)\n",
    "data.foreach(println)[/php]\n",
    "\n",
    "Note – The above code will parallelize the Array of String. It will then map each word with count 1, then reduceByKey will merge the count of values having the similar key.\n",
    "Read: Various Features of RDD\n",
    "\n",
    "3.11. sortByKey()\n",
    "When we apply the sortByKey() function on a dataset of (K, V) pairs, the data is sorted according to the key K in another RDD.\n",
    "\n",
    "sortByKey() example:\n",
    "\n",
    "[php] val data = spark.sparkContext.parallelize(Seq((“maths”,52), (“english”,75), (“science”,82), (“computer”,65), (“maths”,85)))\n",
    "val sorted = data.sortByKey()\n",
    "sorted.foreach(println)[/php]\n",
    "\n",
    "Note – In above code, sortByKey() transformation sort the data RDD into Ascending order of the Key(String).\n",
    "Read: Limitations of RDD\n",
    "\n",
    "3.12. join()\n",
    "The Join is database term. It combines the fields from two table using common values. join() operation in Spark is defined on pair-wise RDD. Pair-wise RDDs are RDD in which each element is in the form of tuples. Where the first element is key and the second element is the value.\n",
    "\n",
    "The boon of using keyed data is that we can combine the data together. The join() operation combines two data sets on the basis of the key.\n",
    "\n",
    "Join() example:\n",
    "\n",
    "[php]val data = spark.sparkContext.parallelize(Array((‘A’,1),(‘b’,2),(‘c’,3)))\n",
    "val data2 =spark.sparkContext.parallelize(Array((‘A’,4),(‘A’,6),(‘b’,7),(‘c’,3),(‘c’,8)))\n",
    "val result = data.join(data2)\n",
    "println(result.collect().mkString(“,”))[/php]\n",
    "\n",
    "Note –  The join() transformation will join two different RDDs on the basis of Key.\n",
    "Read: RDD lineage in Spark: ToDebugString Method\n",
    "\n",
    "3.13. coalesce()\n",
    "To avoid full shuffling of data we use coalesce() function. In coalesce() we use existing partition so that less data is shuffled. Using this we can cut the number of the partition. Suppose, we have four nodes and we want only two nodes. Then the data of extra nodes will be kept onto nodes which we kept.\n",
    "\n",
    "Coalesce() example:\n",
    "\n",
    "[php]val rdd1 = spark.sparkContext.parallelize(Array(“jan”,”feb”,”mar”,”april”,”may”,”jun”),3)\n",
    "val result = rdd1.coalesce(2)\n",
    "result.foreach(println)[/php]\n",
    "\n",
    "Note – The coalesce will decrease the number of partitions of the source RDD to numPartitions define in coalesce argument.\n",
    "4. \n",
    "An action is one of the ways of sending data from Executer to the driver. \n",
    "Executors are agents that are responsible for executing a task. \n",
    "While the driver is a JVM process that coordinates workers and execution of the task. \n",
    "Some of the actions of Spark are:\n",
    "\n",
    "4.1. count()\n",
    "Action count() returns the number of elements in RDD.\n",
    "\n",
    "For example, RDD has values {1, 2, 2, 3, 4, 5, 5, 6} in this RDD “rdd.count()” will give the result 8.\n",
    "\n",
    "Count() example:\n",
    "\n",
    "[php]val data = spark.read.textFile(“spark_test.txt”).rdd\n",
    "val mapFile = data.flatMap(lines => lines.split(” “)).filter(value => value==”spark”)\n",
    "println(mapFile.count())[/php]\n",
    "\n",
    "Note – In above code flatMap() function maps line into words and count the word “Spark” using count() Action after filtering lines containing “Spark” from mapFile.\n",
    "Learn: Spark Streaming\n",
    "\n",
    "4.2. collect()\n",
    "The action collect() is the common and simplest operation that returns our entire RDDs content to driver program. The application of collect() is unit testing where the entire RDD is expected to fit in memory. As a result, it makes easy to compare the result of RDD with the expected result.\n",
    "Action Collect() had a constraint that all the data should fit in the machine, and copies to the driver.\n",
    "\n",
    "Collect() example:\n",
    "\n",
    "[php]val data = spark.sparkContext.parallelize(Array((‘A’,1),(‘b’,2),(‘c’,3)))\n",
    "val data2 =spark.sparkContext.parallelize(Array((‘A’,4),(‘A’,6),(‘b’,7),(‘c’,3),(‘c’,8)))\n",
    "val result = data.join(data2)\n",
    "println(result.collect().mkString(“,”))[/php]\n",
    "\n",
    "Note – join() transformation in above code will join two RDDs on the basis of same key(alphabet). After that collect() action will return all the elements to the dataset as an Array.\n",
    "4.3. take(n)\n",
    "The action take(n) returns n number of elements from RDD. It tries to cut the number of partition it accesses, so it represents a biased collection. We cannot presume the order of the elements.\n",
    "\n",
    "For example, consider RDD {1, 2, 2, 3, 4, 5, 5, 6} in this RDD “take (4)” will give result { 2, 2, 3, 4}\n",
    "\n",
    "Take() example:\n",
    "\n",
    "[php]val data = spark.sparkContext.parallelize(Array((‘k’,5),(‘s’,3),(‘s’,4),(‘p’,7),(‘p’,5),(‘t’,8),(‘k’,6)),3)\n",
    "\n",
    "val group = data.groupByKey().collect()\n",
    "\n",
    "val twoRec = result.take(2)\n",
    "\n",
    "twoRec.foreach(println)[/php]\n",
    "\n",
    "Note – The take(2) Action will return an array with the first n elements of the data set defined in the taking argument.\n",
    "Learn: Apache Spark DStream (Discretized Streams)\n",
    "\n",
    "4.4. top()\n",
    "If ordering is present in our RDD, then we can extract top elements from our RDD using top(). Action top() use default ordering of data.\n",
    "\n",
    "Top() example:\n",
    "\n",
    "[php]val data = spark.read.textFile(“spark_test.txt”).rdd\n",
    "val mapFile = data.map(line => (line,line.length))\n",
    "val res = mapFile.top(3)\n",
    "res.foreach(println)[/php]\n",
    "\n",
    "Note – map() operation will map each line with its length. And top(3) will return 3 records from mapFile with default ordering.\n",
    "4.5. countByValue()\n",
    "The countByValue() returns, many times each element occur in RDD.\n",
    "\n",
    "For example, RDD has values {1, 2, 2, 3, 4, 5, 5, 6} in this RDD “rdd.countByValue()”  will give the result {(1,1), (2,2), (3,1), (4,1), (5,2), (6,1)}\n",
    "\n",
    "countByValue() example:\n",
    "\n",
    "[php]val data = spark.read.textFile(“spark_test.txt”).rdd\n",
    "val result= data.map(line => (line,line.length)).countByValue()\n",
    "result.foreach(println)[/php]\n",
    "\n",
    "Note – The countByValue() action will return a hashmap of (K, Int) pairs with the count of each key.\n",
    "Learn: Apache Spark Streaming Transformation Operations\n",
    "\n",
    "4.6. reduce()\n",
    "The reduce() function takes the two elements as input from the RDD and then produces the output of the same type as that of the input elements. The simple forms of such function are an addition. We can add the elements of RDD, count the number of words. It accepts commutative and associative operations as an argument.\n",
    "\n",
    "Reduce() example:\n",
    "\n",
    "[php]val rdd1 = spark.sparkContext.parallelize(List(20,32,45,62,8,5))\n",
    "val sum = rdd1.reduce(_+_)\n",
    "println(sum)[/php]\n",
    "\n",
    "Note – The reduce() action in above code will add the elements of the source RDD.\n",
    "4.7. fold()\n",
    "The signature of the fold() is like reduce(). Besides, it takes “zero value” as input, which is used for the initial call on each partition. But, the condition with zero value is that it should be the identity element of that operation. The key difference between fold() and reduce() is that, reduce() throws an exception for empty collection, but fold() is defined for empty collection.\n",
    "\n",
    "For example, zero is an identity for addition; one is identity element for multiplication. The return type of fold() is same as that of the element of RDD we are operating on.\n",
    "For example, rdd.fold(0)((x, y) => x + y).\n",
    "\n",
    "Fold() example:\n",
    "\n",
    "[php]val rdd1 = spark.sparkContext.parallelize(List((“maths”, 80),(“science”, 90)))\n",
    "val additionalMarks = (“extra”, 4)\n",
    "val sum = rdd1.fold(additionalMarks){ (acc, marks) => val add = acc._2 + marks._2\n",
    "(“total”, add)\n",
    "}\n",
    "println(sum)[/php]\n",
    "\n",
    "Note – In above code additionalMarks is an initial value. This value will be added to the int value of each record in the source RDD.\n",
    "Learn: Spark Streaming Checkpoint in Apache Spark\n",
    "\n",
    "4.8. aggregate()\n",
    "It gives us the flexibility to get data type different from the input type. The aggregate() takes two functions to get the final result. Through one function we combine the element from our RDD with the accumulator, and the second, to combine the accumulator. Hence, in aggregate, we supply the initial zero value of the type which we want to return.\n",
    "\n",
    "4.9. foreach()\n",
    "When we have a situation where we want to apply operation on each element of RDD, but it should not return value to the driver. In this case, foreach() function is useful. For example, inserting a record into the database.\n",
    "\n",
    "Foreach() example:\n",
    "\n",
    "[php]val data = spark.sparkContext.parallelize(Array((‘k’,5),(‘s’,3),(‘s’,4),(‘p’,7),(‘p’,5),(‘t’,8),(‘k’,6)),3)\n",
    "val group = data.groupByKey().collect()\n",
    "group.foreach(println)[/php]\n",
    "\n",
    "Note – The foreach() action run a function (println) on each element of the dataset group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303295ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 4. Spark SQL and Data Source Integration:\n",
    "\n",
    "a) Write a Python program to connect Spark with a relational database (e.g., MySQL, PostgreSQL).\n",
    "\n",
    "Solution:- Connecting PySpark application with a locally installed MySQL database\n",
    "Before writing your scripts you need to put the MySQL jar file inside jars directory\n",
    "/home/<username>/.local/lib/python3.8/site-packages/pyspark/jars/\n",
    "mysql-connector-java-8.0.22.jar\n",
    "\n",
    "Download the jar file from the below link\n",
    "\n",
    "Download mysql-connector-java JAR file with all dependencies\n",
    "JDBC Type 4 driver for MySQL Artifact mysql-connector-java Group org.wisdom-framework Version 5.1.34_1 Last update 13…\n",
    "jar-download.com\n",
    "\n",
    "And configured spark session\n",
    "\n",
    "spark = SparkSession \\\n",
    "   .builder.config(\"spark.jars\", \"mysql-connector-java-8.0.22.jar\")\\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"PySpark_MySQL_test\") \\\n",
    "   .getOrCreate()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder.config(\"spark.jars\", \"mysql-connector-java-8.0.22.jar\") \\\n",
    "    .master(\"local\").\\\n",
    "    appName(\"PySpark_MySQL_test\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "jdbcDF = spark.read.format(\"jdbc\").\\\n",
    "    option(\n",
    "            url=\"jdbc:mysql://localhost/<database_name>\",\n",
    "            driver=\"com.mysql.jdbc.Driver\",\n",
    "            dbtable=\"<table_name>\",\n",
    "            user=\"<user_name>\"\n",
    "            password=\"<password>\").\\\n",
    "load()\n",
    "jdbcDF.show()\n",
    "\n",
    "\n",
    "\n",
    "Connecting PySpark with a locally installed Postgres RDB\n",
    "Here you need to put the Postgres jar file inside the jars directory\n",
    "/home/<username>/.local/lib/python3.8/site-packages/pyspark/jars/\n",
    "postgresql-42.2.14.jar\n",
    "\n",
    "Download the jar file from the below link\n",
    "\n",
    "And configured spark session\n",
    "\n",
    "spark = SparkSession \\\n",
    "      .builder \\\n",
    "      .appName(\"Python Spark SQL basic example\") \\ \n",
    "      .config(\"spark.jars\", \"postgresql-42.2.14.jar\") \\\n",
    "      .getOrCreate()\n",
    "Download\n",
    "Binary JAR file downloads of the JDBC driver are available here and the current version with Maven Repository. Because…\n",
    "jdbc.postgresql.org\n",
    "\n",
    "\n",
    "rom pyspark.sql import SparkSession\n",
    "\n",
    "# the Spark session should be instantiated as follows\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.jars\", \"postgresql-42.2.14.jar\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "jdbcDF = spark.read.format(\"jdbc\").\\\n",
    "options(\n",
    "         url='jdbc:postgresql://localhost:5432/<database_name>', \n",
    "         dbtable='<table_name>',\n",
    "         user='<user_name>',\n",
    "         password='<user_password>',\n",
    "         driver='org.postgresql.Driver').\\\n",
    "load()\n",
    "\n",
    "Connecting PySpark with Postgres RDS\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.jars\", \"postgresql-42.2.14.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "jdbcDF = spark.read.format(\"jdbc\").\\\n",
    "options(\n",
    "         url='jdbc:postgresql://nbj-instance.crjyrm6gyucn.-1.rds.amazonaws.com/<database_name>', \n",
    "         dbtable='<table_name>',\n",
    "         user='<user_name>',\n",
    "         password='<password>',\n",
    "         driver='org.postgresql.Driver').\\\n",
    "load()\n",
    "jdbcDF.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "b)Perform SQL operations on the data stored in the database using Spark SQL.\n",
    "\n",
    "Solution:- SQL module.\n",
    "\n",
    "spark.sql.SparkSession – SparkSession is the main entry point for DataFrame and SQL functionality.\n",
    "spark.sql.DataFrame – DataFrame is a distributed collection of data organized into named columns.\n",
    "spark.sql.Column – A column expression in a DataFrame.\n",
    "spark.sql.Row – A row of data in a DataFrame.\n",
    "spark.sql.GroupedData – An object type that is returned by DataFrame.groupBy().\n",
    "spark.sql.DataFrameNaFunctions – Methods for handling missing data (null values).\n",
    "spark.sql.DataFrameStatFunctions – Methods for statistics functionality.\n",
    "spark.sql.functions – List of standard built-in functions.\n",
    "spark.sql.types – Available SQL data types in Spark.\n",
    "spark.sql.Window – Would be used to work with window functions.\n",
    "Regardless of what approach you use, you have to create a SparkSession which is an entry point to the Spark application.\n",
    "\n",
    "3. Running SQL Queries in Spark\n",
    "Spark SQL is one of the most used Spark modules which is used for processing structured columnar data format. Once you have a DataFrame created, you can interact with the data by using SQL syntax.\n",
    "\n",
    "In other words, Spark SQL brings native RAW SQL queries on Spark meaning you can run traditional ANSI SQL on Spark Dataframe, in the SQL tutorial, you will learn in detail using SQL select, where, group by, join, union e.t.c\n",
    "\n",
    "In order to use SQL, first, create a temporary table on DataFrame using the createOrReplaceTempView() function. Once created, this table can be accessed throughout the SparkSession using sql() and it will be dropped along with your SparkContext termination.\n",
    "\n",
    "Use sql() method of the SparkSession object to run the query and this method returns a new DataFrame.\n",
    "\n",
    "4. Spark SQL Examples\n",
    "4.1 Create SQL View\n",
    "Create a DataFrame from a CSV file. You can find this CSV file at Github project.\n",
    "\n",
    "\n",
    "// Read CSV file into table\n",
    "val df = spark.read.option(\"header\",true) \n",
    "          .csv(\"/Users/admin/simple-zipcodes.csv\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "To use ANSI SQL query similar to RDBMS, you need to create a temporary table by reading the data from a CSV file. \n",
    "\n",
    "// Read CSV file into table\n",
    "spark.read.option(\"header\",true) \n",
    "          .csv(\"/Users/admin/simple-zipcodes.csv\") \n",
    "          .createOrReplaceTempView(\"Zipcodes\")\n",
    "        \n",
    "        \n",
    "        Spark SQL to Select Columns\n",
    "The select() function of DataFrame API is used to select the specific columns from the DataFrame.\n",
    "\n",
    "\n",
    "// DataFrame API Select query\n",
    "df.select(\"country\",\"city\",\"zipcode\",\"state\") \n",
    "     .show(5)\n",
    "In SQL, you can achieve the same using SELECT FROM clause as shown below.\n",
    "\n",
    "\n",
    "// SQL Select query\n",
    "spark.sql(\"SELECT country, city, zipcode, state FROM ZIPCODES\") \n",
    "     .show(5)\n",
    "Both above examples yields the below output.\n",
    "\n",
    "spark sql\n",
    "\n",
    "Filter Rows\n",
    "To filter the rows from the data, you can use where() function from the DataFrame API.\n",
    "\n",
    "\n",
    "// DataFrame API where()\n",
    "df.select(\"country\",\"city\",\"zipcode\",\"state\") \n",
    "  .where(\"state == 'AZ'\") \n",
    "  .show(5)\n",
    "Similarly, in SQL you can use WHERE clause as follows.\n",
    "\n",
    "\n",
    "// SQL where\n",
    "spark.sql(\"\"\" SELECT  country, city, zipcode, state FROM ZIPCODES \n",
    "          WHERE state = 'AZ' \"\"\") \n",
    "     .show(5)\n",
    "\n",
    "spark sql example\n",
    "\n",
    "Sorting\n",
    "To sort rows on a specific column use orderBy() function on DataFrame API.\n",
    "\n",
    "\n",
    "\n",
    "// sorting\n",
    "df.select(\"country\",\"city\",\"zipcode\",\"state\") \n",
    "  .where(\"state in ('PR','AZ','FL')\") \n",
    "  .orderBy(\"state\") \n",
    "  .show(10)\n",
    "In SQL, you can achieve sorting by using ORDER BY clause.\n",
    "\n",
    "\n",
    "// SQL ORDER BY\n",
    "spark.sql(\"\"\" SELECT  country, city, zipcode, state FROM ZIPCODES \n",
    "          WHERE state in ('PR','AZ','FL') order by state \"\"\") \n",
    "     .show(10)\n",
    "\n",
    "Grouping\n",
    "The groupBy().count() is used to perform the group by on DataFrame.\n",
    "\n",
    "\n",
    "// grouping\n",
    "df.groupBy(\"state\").count() \n",
    "  .show()\n",
    "You can achieve group by in Spark SQL is by using GROUP BY clause.\n",
    "\n",
    "\n",
    "\n",
    "// SQL GROUP BY clause\n",
    "spark.sql(\"\"\" SELECT state, count(*) as count FROM ZIPCODES \n",
    "          GROUP BY state\"\"\") \n",
    "     .show()\n",
    "\n",
    "SQL Join Operations\n",
    "Similarly, if you have two tables, you can perform the Join operations in Spark.\n",
    "Spark DataFrame supports all basic SQL Join Types like INNER, LEFT OUTER, RIGHT OUTER, LEFT ANTI, LEFT SEMI, CROSS, SELF JOIN.\n",
    "Spark SQL Joins are wider transformations that result in data shuffling over the network hence they have huge performance issues when not designed with care.\n",
    "\n",
    "On the other hand Spark SQL Joins comes with more optimization by default (thanks to DataFrames & Dataset) \n",
    "however still there would be some performance issues to consider while using.\n",
    "2. Inner Join\n",
    "Spark Inner join is the default join and it’s mostly used, It is used to join two DataFrames/Datasets on key columns, and where keys don’t match the rows get dropped from both datasets (emp & dept).\n",
    "\n",
    "\n",
    "  empDF.join(deptDF,empDF(\"emp_dept_id\") ===  deptDF(\"dept_id\"),\"inner\")\n",
    "    .show(false)\n",
    "When we apply Inner join on our datasets, It drops “emp_dept_id” 50 from “emp” and “dept_id” 30 from “dept” datasets. Below is the result of the above Join expression.\n",
    "\n",
    "\n",
    "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
    "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
    "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
    "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
    "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
    "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
    "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
    "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
    "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
    "3. Full Outer Join\n",
    "Outer a.k.a full, fullouter join returns all rows from both Spark DataFrame/Datasets, where join expression doesn’t match it returns null on respective record columns.\n",
    "\n",
    "\n",
    "  empDF.join(deptDF,empDF(\"emp_dept_id\") ===  deptDF(\"dept_id\"),\"outer\")\n",
    "    .show(false)\n",
    "  empDF.join(deptDF,empDF(\"emp_dept_id\") ===  deptDF(\"dept_id\"),\"full\")\n",
    "    .show(false)\n",
    "  empDF.join(deptDF,empDF(\"emp_dept_id\") ===  deptDF(\"dept_id\"),\"fullouter\")\n",
    "    .show(false)\n",
    "From our “emp” dataset’s “emp_dept_id” with value 50 doesn’t have a record on “dept” hence dept columns have null and “dept_id” 30 doesn’t have a record in “emp” hence you see null’s on emp columns. Below is the result of the above Join expression.\n",
    "\n",
    "\n",
    "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
    "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
    "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
    "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
    "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
    "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
    "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
    "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
    "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
    "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
    "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
    "4. Left Outer Join\n",
    "Spark Left a.k.a Left Outer join returns all rows from the left DataFrame/Dataset regardless of match found on the right dataset when join expression doesn’t match, it assigns null for that record and drops records from right where match not found.\n",
    "\n",
    "\n",
    "  empDF.join(deptDF,empDF(\"emp_dept_id\") ===  deptDF(\"dept_id\"),\"left\")\n",
    "    .show(false)\n",
    "  empDF.join(deptDF,empDF(\"emp_dept_id\") ===  deptDF(\"dept_id\"),\"leftouter\")\n",
    "    .show(false)\n",
    "From our dataset, “emp_dept_id” 5o doesn’t have a record on “dept” dataset hence, this record contains null on “dept” columns (dept_name & dept_id). and “dept_id” 30 from “dept” dataset dropped from the results. Below is the result of the above Join expression.\n",
    "\n",
    "\n",
    "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
    "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
    "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
    "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
    "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
    "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
    "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
    "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
    "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
    "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
    "5. Right Outer Join\n",
    "Spark Right a.k.a Right Outer join is opposite of left join, here it returns all rows from the right DataFrame/Dataset regardless of math found on the left dataset, when join expression doesn’t match, it assigns null for that record and drops records from left where match not found.\n",
    "\n",
    "\n",
    "  empDF.join(deptDF,empDF(\"emp_dept_id\") ===  deptDF(\"dept_id\"),\"right\")\n",
    "   .show(false)\n",
    "  empDF.join(deptDF,empDF(\"emp_dept_id\") ===  deptDF(\"dept_id\"),\"rightouter\")\n",
    "   .show(false)\n",
    "From our example, the right dataset “dept_id” 30 doesn’t have it on the left dataset “emp” hence, this record contains null on “emp” columns. and “emp_dept_id” 50 dropped as a match not found on left. Below is the result of the above Join expression.\n",
    "\n",
    "\n",
    "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
    "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
    "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
    "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
    "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
    "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
    "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
    "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
    "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
    "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
    "6. Left Semi Join\n",
    "Spark Left Semi join is similar to inner join difference being leftsemi join returns all columns from the left DataFrame/Dataset and ignores all columns from the right dataset. In other words, this join returns columns from the only left dataset for the records match in the right dataset on join expression, records not matched on join expression are ignored from both left and right datasets.\n",
    "\n",
    "The same result can be achieved using select on the result of the inner join however, using this join would be efficient.\n",
    "\n",
    "\n",
    "  empDF.join(deptDF,empDF(\"emp_dept_id\") ===  deptDF(\"dept_id\"),\"leftsemi\")\n",
    "    .show(false)\n",
    "Below is the result of the above join expression.\n",
    "\n",
    "\n",
    "leftsemi join\n",
    "+------+--------+---------------+-----------+-----------+------+------+\n",
    "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
    "+------+--------+---------------+-----------+-----------+------+------+\n",
    "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
    "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
    "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
    "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
    "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
    "+------+--------+---------------+-----------+-----------+------+------+\n",
    "7. Left Anti Join\n",
    "Left Anti join does the exact opposite of the Spark leftsemi join, leftanti join returns only columns from the left DataFrame/Dataset for non-matched records.\n",
    "\n",
    "\n",
    "  empDF.join(deptDF,empDF(\"emp_dept_id\") ===  deptDF(\"dept_id\"),\"leftanti\")\n",
    "    .show(false)\n",
    "Yields below output\n",
    "\n",
    "\n",
    "+------+-----+---------------+-----------+-----------+------+------+\n",
    "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
    "+------+-----+---------------+-----------+-----------+------+------+\n",
    "|6     |Brown|2              |2010       |50         |      |-1    |\n",
    "+------+-----+---------------+-----------+-----------+------+------+\n",
    "8. Self Join\n",
    "Spark Joins are not complete without a self join, Though there is no self-join type available, we can use any of the above-explained join types to join DataFrame to itself. below example use inner self join\n",
    "\n",
    "\n",
    "  empDF.as(\"emp1\").join(empDF.as(\"emp2\"),\n",
    "    col(\"emp1.superior_emp_id\") === col(\"emp2.emp_id\"),\"inner\")\n",
    "    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"),\n",
    "      col(\"emp2.emp_id\").as(\"superior_emp_id\"),\n",
    "      col(\"emp2.name\").as(\"superior_emp_name\"))\n",
    "      .show(false)\n",
    "Here, we are joining emp dataset with itself to find out superior emp_id and name for all employees.\n",
    "\n",
    "\n",
    "+------+--------+---------------+-----------------+\n",
    "|emp_id|name    |superior_emp_id|superior_emp_name|\n",
    "+------+--------+---------------+-----------------+\n",
    "|2     |Rose    |1              |Smith            |\n",
    "|3     |Williams|1              |Smith            |\n",
    "|4     |Jones   |2              |Rose             |\n",
    "|5     |Brown   |2              |Rose             |\n",
    "|6     |Brown   |2              |Rose             |\n",
    "+------+--------+---------------+-----------------+\n",
    "9. Using SQL Expression\n",
    "Since Spark SQL support native SQL syntax, we can also write join operations after creating temporary tables on DataFrame’s and using spark.sql()\n",
    "\n",
    "\n",
    "  empDF.createOrReplaceTempView(\"EMP\")\n",
    "  deptDF.createOrReplaceTempView(\"DEPT\")\n",
    "//SQL JOIN\n",
    "  val joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\")\n",
    "  joinDF.show(false)\n",
    "\n",
    "  val joinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\")\n",
    "  joinDF2.show(false)\n",
    "\n",
    "\n",
    " Union\n",
    "    \n",
    "Dataframe union() – union() method of the DataFrame is used to combine two DataFrame’s of the same structure/schema. If schemas are not the same it returns an error.\n",
    "\n",
    "DataFrame unionAll() – unionAll() is deprecated since Spark “2.0.0” version and replaced with union().\n",
    "\n",
    "Note: In other SQL’s, Union eliminates the duplicates but UnionAll combines two datasets including duplicate records. \n",
    "But, in spark both behave the same and use DataFrame duplicate function to remove duplicate rows.\n",
    "\n",
    "DataFrame unionAll() – unionAll() is deprecated since Spark “2.0.0” version and replaced with union().\n",
    "\n",
    "Note: In other SQL’s, Union eliminates the duplicates but UnionAll combines two datasets including duplicate records. But, in spark both behave the same and use DataFrame duplicate function to remove duplicate rows.\n",
    "    \n",
    "\n",
    "root\n",
    " |-- employee_name: string (nullable = true)\n",
    " |-- department: string (nullable = true)\n",
    " |-- state: string (nullable = true)\n",
    " |-- salary: integer (nullable = false)\n",
    " |-- age: integer (nullable = false)\n",
    " |-- bonus: integer (nullable = false)\n",
    "\n",
    "+-------------+----------+-----+------+---+-----+\n",
    "|employee_name|department|state|salary|age|bonus|\n",
    "+-------------+----------+-----+------+---+-----+\n",
    "|        James|     Sales|   NY| 90000| 34|10000|\n",
    "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
    "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
    "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
    "+-------------+----------+-----+------+---+-----+\n",
    "\n",
    "Now, let’s create a second Dataframe with the new records and some records from the above Dataframe but with the same schema.\n",
    "\n",
    "\n",
    "  val simpleData2 = Seq((\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  )\n",
    "  val df2 = simpleData2.toDF(\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\")\n",
    "This yields below output\n",
    "\n",
    "\n",
    "+-------------+----------+-----+------+---+-----+\n",
    "|employee_name|department|state|salary|age|bonus|\n",
    "+-------------+----------+-----+------+---+-----+\n",
    "|James        |Sales     |NY   |90000 |34 |10000|\n",
    "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
    "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
    "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
    "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
    "+-------------+----------+-----+------+---+-----+\n",
    "\n",
    "Combine two or more DataFrames using union\n",
    "DataFrame union() method combines two DataFrames and returns the new DataFrame with all rows from two Dataframes regardless of duplicate data.\n",
    "\n",
    "\n",
    "  val df3 = df.union(df2)\n",
    "  df3.show(false)\n",
    "As you see below it returns all records.\n",
    "\n",
    "\n",
    "+-------------+----------+-----+------+---+-----+\n",
    "|employee_name|department|state|salary|age|bonus|\n",
    "+-------------+----------+-----+------+---+-----+\n",
    "|James        |Sales     |NY   |90000 |34 |10000|\n",
    "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
    "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
    "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
    "|James        |Sales     |NY   |90000 |34 |10000|\n",
    "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
    "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
    "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
    "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
    "+-------------+----------+-----+------+---+-----+\n",
    "Combine DataFrames using unionAll\n",
    "DataFrame unionAll() method is deprecated since Spark “2.0.0” version and recommends using the union() method.\n",
    "\n",
    "\n",
    "\n",
    "  val df4 = df.unionAll(df2)\n",
    "  df4.show(false)\n",
    "Returns the same output as above.\n",
    "\n",
    "Combine without Duplicates\n",
    "Since the union() method returns all rows without distinct records, we will use the distinct() function to return just one record when duplicate exists.\n",
    "\n",
    "\n",
    "  val df5 = df.union(df2).distinct()\n",
    "  df5.show(false)\n",
    "Yields below output. As you see, this returns only distinct rows.\n",
    "\n",
    "\n",
    "+-------------+----------+-----+------+---+-----+\n",
    "|employee_name|department|state|salary|age|bonus|\n",
    "+-------------+----------+-----+------+---+-----+\n",
    "|James        |Sales     |NY   |90000 |34 |10000|\n",
    "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
    "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
    "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
    "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
    "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
    "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
    "+-------------+----------+-----+------+---+-----+\n",
    "Complete Example of DataFrame Union\n",
    "\n",
    "package com.sparkbyexamples.spark.dataframe\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "object UnionExample extends App{\n",
    "\n",
    "  val spark: SparkSession = SparkSession.builder()\n",
    "    .master(\"local[1]\")\n",
    "    .appName(\"SparkByExamples.com\")\n",
    "    .getOrCreate()\n",
    "\n",
    "  spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "  import spark.implicits._\n",
    "\n",
    "  val simpleData = Seq((\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000)\n",
    "  )\n",
    "  val df = simpleData.toDF(\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\")\n",
    "  df.printSchema()\n",
    "  df.show()\n",
    "\n",
    "  val simpleData2 = Seq((\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  )\n",
    "  val df2 = simpleData2.toDF(\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\")\n",
    "  df2.show(false)\n",
    "\n",
    "  val df3 = df.union(df2)\n",
    "  df3.show(false)\n",
    "  df3.distinct().show(false)\n",
    "\n",
    "  val df4 = df.unionAll(df2)\n",
    "  df4.show(false)\n",
    "}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd55e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3.\n",
    "\n",
    "Solution:- First, Spark is intended to enhance, not replace, the Hadoop stack. From day one, Spark was designed to read and write data from and to HDFS, as well as other storage systems, such as HBase and Amazon’s S3. As such, Hadoop users can enrich their processing capabilities by combining Spark with Hadoop MapReduce, HBase, and other big data frameworks.\n",
    "\n",
    "Second, we have constantly focused on making it as easy as possible for every Hadoop user to take advantage of Spark’s capabilities. No matter whether you run Hadoop 1.x or Hadoop 2.0 (YARN), and no matter whether you have administrative privileges to configure the Hadoop cluster or not, there is a way for you to run Spark! In particular, there are three ways to deploy Spark in a Hadoop cluster: standalone, YARN, and SIMR.\n",
    "\n",
    "Standalone deployment: With the standalone deployment one can statically allocate resources on all or a subset of machines in a Hadoop cluster and run Spark side by side with Hadoop MR. The user can then run arbitrary Spark jobs on her HDFS data. Its simplicity makes this the deployment of choice for many Hadoop 1.x users.\n",
    "\n",
    "Hadoop Yarn deployment: Hadoop users who have already deployed or are planning to deploy Hadoop Yarn can simply run Spark on YARN without any pre-installation or administrative access required. This allows users to easily integrate Spark in their Hadoop stack and take advantage of the full power of Spark, as well as of other components running on top of Spark.\n",
    "\n",
    "Spark In MapReduce (SIMR): For the Hadoop users that are not running YARN yet, another option, in addition to the standalone deployment, is to use SIMR to launch Spark jobs inside MapReduce. With SIMR, users can start experimenting with Spark and use its shell within a couple of minutes after downloading it! This tremendously lowers the barrier of deployment, and lets virtually everyone play with Spark.\n",
    "    \n",
    "    \n",
    " Apache Spark AWS S3 Datasource\n",
    "\n",
    "example will be to read a simple CSV file from our local disk and write it to S3. \n",
    "Then we will read, what we have just written, from S3 again and print it in the notebook/console.\n",
    "\n",
    "Assumptions\n",
    "Your AWS S3 bucket has been created, you have accessKeyId and secretAccessKey.\n",
    "\n",
    "Coding\n",
    "Open up a Jupyter notebook and put the following to the first paragraph to find findspark.\n",
    "\n",
    "import findspark\n",
    "# /opt/manual/spark: this is SPARK_HOME path\n",
    "findspark.init(“/opt/manual/spark”)\n",
    "Download standard PySpark libs\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark import SparkConf, SparkContext\n",
    "Libraries (Jars) for S3 Connection\n",
    "Now here we will download the additional libraries (jar files) to be used in the S3 connection and put them among the other jar files of Spark. Of course, there are other ways to do this. But here I prefer this way.\n",
    "\n",
    "When downloading files from the Maven repo, you should definitely pay attention to the spark-hadoop version (You decide this while downloading and installing spark and you choose among many compilations, my example is Spark 3.0.0. Hadoop 3.2). To prevent any mistake please pay attention to the following figures and explanations.\n",
    "\n",
    "\n",
    "\n",
    "Downloading and moving jar files:\n",
    "There are other jar files in spark installation. We will put these two into the `SPARK_HOME/jars` directory.\n",
    "\n",
    "wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar\n",
    "wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.375/aws-java-sdk-bundle-1.11.375.jar\n",
    "mv aws-java-sdk-bundle-1.11.375.jar hadoop-aws-3.2.0.jar /opt/manual/spark/jars/\n",
    "Spark Session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.verision\n",
    "Output\n",
    "3.0.0\n",
    "Configuration\n",
    "Let’s first assign access keys to variables. You can also introduce it to environment variables if you want. However, neither send these keys to repos such as github, nor share them with anyone else, if in doubt, revoke the keys and generate new ones.\n",
    "\n",
    "accessKeyId=’your_access_key’\n",
    "secretAccessKey=’your_secret_key’\n",
    "Now let’s define the configurations with a function and add them to the SparkContext:\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(‘fs.s3a.access.key’, accessKeyId)\n",
    "sc._jsc.hadoopConfiguration().set(‘fs.s3a.secret.key’, secretAccessKey)\n",
    "sc._jsc.hadoopConfiguration().set(‘fs.s3a.path.style.access’, ‘true’)\n",
    "sc._jsc.hadoopConfiguration().set(‘fs.s3a.impl’, ‘org.apache.hadoop.fs.s3a.S3AFileSystem’)\n",
    " \n",
    "sc._jsc.hadoopConfiguration().set(‘fs.s3a.endpoint’, ‘s3.amazonaws.com’)\n",
    "Let’s read the dataset. You can access it here.\n",
    "\n",
    "df = spark.read \\\n",
    ".option(“inferSchema”,True) \\\n",
    ".option(“header”, True) \\\n",
    ".csv(“file:///home/train/datasets/simple_data.csv”)\n",
    "Let’s take a look at the dataset:\n",
    "\n",
    "df.show(3)\n",
    "+ — — — + — — -+ — -+ — — — — + — — — — + — — — — — -+\n",
    "|sirano | isim |yas | meslek  | sehir   |aylik_gelir |\n",
    "+ — — — + — — -+ — -+ — — — — + — — — — + — — — — — -+\n",
    "| 1     |Cemal | 35 | Isci    | Ankara  | 3500       |\n",
    "| 2     |Ceyda | 42 | Memur   | Kayseri | 4200       |\n",
    "| 3     |Timur | 30 |Müzisyen |Istanbul | 9000       |\n",
    "+ — — — + — — -+ — -+ — — — — + — — — — + — — — — — -+\n",
    "Writing Spark Dataframe to AWS S3\n",
    "\n",
    "After downloading the libraries with the necessary and appropriate versions above and configuring Spark, the work is no different than writing to the local disk at this point.\n",
    "\n",
    "df.write.format(‘csv’).option(‘header’,’true’) \\\n",
    ".save(‘s3a://<your_bucket_name_here>/<your_folder_here>’, mode=’overwrite’)\n",
    "\n",
    "Reading Data from AWS S3 with Spark\n",
    "Now let’s read this data again with Spark.\n",
    "\n",
    "df_s3 = spark.read.format(‘csv’).option(‘header’,’true’) \\\n",
    ".load(‘s3a://<your_bucket_name_here>/<your_folder_here>’)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
